
### softmax回归
![image.png](https://cdn.jsdelivr.net/gh/Thomas333333/MyPostImage/Images/20240111202920.png)

$$
\hat{\mathbf{y}}=\mathrm{softmax}(\mathbf{o})\quad\text{其中}\quad\hat{y}_j=\frac{\exp(o_j)}{\sum_k\exp(o_k)}
$$

### 对数似然

$$
P(\mathbf{Y}\mid\mathbf{X})=\prod_{i=1}^nP(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)}).
$$


根据最大似然估计，我们最大化$P(\mathcal{Y\mid X})$,相当于最小化负对数似然：

$$
-\log P(\mathbf{Y}\mid\mathbf{X})=\sum_{i=1}^{n}-\log P(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)})=\sum_{i=1}^{n}l(\mathbf{y}^{(i)},\mathbf{\hat{y}}^{(i)}),
$$

其中，对于任何标签y和模型预测$\hat{\mathbf{y}}$,损失函数（交叉熵损失）为：

$$
l(\mathbf{y},\hat{\mathbf{y}})=-\sum_{j=1}^{q}y_{j}\log\hat{y}_{j}.
$$

### 熵
信息论的核心思想是量化数据中的信息内容。在信息论中，该数值被称为分布$P$的熵(entropy)。可以通过以下方程得到：


$$
H[P]=\sum_{j}-P(j)\log P(j).
$$

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到“惊异”。 克劳德·香农决定用信息量$\log\frac{1}{P(j)}=-\log P(j)$来量化这种惊异程度。 在观察一个事件$j$时，并赋予它(主观)概率$P(j)$ 当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。 在中定义的熵，是当分配的概率真正匹配数据生成过程时的**信息量的期望**。









