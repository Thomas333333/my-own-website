	
时间：24.1.8

## 1.引言

 Static LM➡️ Network LM ➡️Pre-trained LM ➡️ Large LM

PLM:预训练一个网络捕捉上下文的感知词表示。
LLM:扩展模型大小或数据大小获得涌现能力。

IR领域正在受到聊天机器人ChatGPT所带来的新挑战。New Bing展示了一个初步的增强搜索结果的尝试。

本综述从四个主要方面对 LLM 的最近进展进行文献综述，包括预训练（如何预训练出一个有能力的 LLM）、适配微调（如何从有效性和安全性两个角度有效地微调预训练的 LLM）、使用（如何利用 LLM 解决各种下游任务）以及能力评估（如何评估 LLM 的能力和现有的经验性发现）。

## 2.概述

### 2.1 涌现能力
+ 上下文学习（类比学习）：根据给出的示例完成填空
+ 指令微调：自然语言数据集上微调，使其在未见过的任务上也有较好表现
+ 逐步推理：CoT（思维链）


## 3. 模型资源
### 3.1 公开可用的API
[CodeGen（11B）](https://arxiv.org/abs/2203.13474)是一个为生成代码设计的自回归语言模型，需要LLM获得足够的编程知识。

### 3.2语料库
### 3.3python代码库资源

## 4.预训练
### 4.1数据收集

#### 4.1.1 数据来源
鉴于 LLM 所展现出的惊人泛化能力，有研究将预训练语料库扩展到更专用的数据集，如多语言数
据、科学数据和代码等，以此来赋予 LLM 解决专用任务的能力 [34, 56, 91]。

数学符号和蛋白质序列，通常需要特定的标记化和预处理技术来将这些不同格式的数据转换为可以被语言模型处理的统一形式。

编写的程序可以成功通过专家设计的单元测试用例 [78] 或解决竞赛编程问题 [111]。最近的一项研究 [46] 还推测，训练代码可能是复杂推理能力（例如 CoT 能力 [32]）的来源。此外，将推理任务格式化为代码的形式还可以帮助 LLM 生成更准确的结果 [156, 157]。

#### 4.1.2数据预处理
质量过滤：基于度量的过滤：可以利用生成文本的评估度量，例如困惑度（perplexity），来检测和删除不自然的句子。

#### 4.1.3 预训练数据
预训练数据的数量：建议研究人员在充分训练模型时，尤其是在调节模型参数时，应该更加关注高质量数据的数量。

### 4.2架构

#### 4.2.1主流架构

编码器-解码器架构：编码器采用堆叠的多头自注意层对输入序列进行编码以生成其潜在表示，而解码器对这些表示进行交叉注意并自回归地生成目标序列。目前，只有少数LLM 是基于编码器-解码器架构构建的，例如 Flan-T5。


因果解码器架构（仅解码器）：因果解码器架构采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身。输入和输出 token 通过解码器以相同的方式进行处理。代表模型GPT-3。

