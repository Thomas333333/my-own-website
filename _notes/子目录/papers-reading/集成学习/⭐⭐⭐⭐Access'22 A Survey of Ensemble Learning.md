
论文链接： https://ieeexplore.ieee.org/abstract/document/9893798 

## 1.Introduction 
集成学习主要有
+ boosting 
	+ Gradient boosting, XGBoost, and AdaBoost 
+ bagging 
	+ random forest and Extra Trees classifier 
+ stacking 
	+  super ensemble and blending  techniques 


a review of recent developments and applications of ensemble learning [36]. sentiment analysis [38]

## 2.Overview

集成方法可以分为并行和序列两种类型。并行方法独立训练不同的基分类器，然后使用一个组合器将它们的预测结果结合起来。**一个常用的并行集成方法是 bagging 及其扩展，即随机森林算法 [44]**。并行集成算法利用并行生成基学习器，以促进集成成员之间的多样性。此外，并行集成可以根据基学习器的同质性分为同质和异质两种类型。同质集成由使用相同机器学习算法构建的模型组成，而异质集成则包含来自不同算法的模型

顺序集成并不独立适应基础模型。它们是迭代训练的，因此每次迭代中的模型都学习来修正前一个模型的错误。

ensemble learning技术的成功主要依赖于基础学习器的准确性和多样性 [49]。在实现集成分类器时要使基础模型具有多样性是具有挑战性的。

### 2.1主要组合方法
+ MAJORITY VOTING （多数投票）
+ WEIGHTED MAJORITY VOTING （加权多数投票）

加权多数投票的更全面概述可以在 [57] 中找到。与此同时 ， #idea   **文献中存在多种组合规则。在 [58] 中介绍了一些组合规则，包括最小值、最大值、乘积、中位数和求和规则。**


### 2.2集成选择
+ 集成选择策略用于选择最佳的基础分类器子集，通常会借助评分函数来指导选择过程 [61]。Caruana 等人 [60] 发展了一种前向模型选择策略，用于提取表现最佳的基础模型子集。最近，Guo 等人 [62] 开发了一种方法，通过考虑边际和多样性的评估指标对基础模型进行排序。与此同时，基于优化的技术将选择过程形式化为一个优化问题，可以使用数学规划或启发式优化来解决。
+ Pérez-Gállego 等人 [67] 对集成选择策略进行了详细描述，并对静态和动态选择方法进行了比较。

## 3.集成学习方法

### 3.1 Boosting 

Boosting算法最早在1990年由Schapire [43]在回应Kearns和Valiant [68]提出的一个问题中被讨论，问题是关于一组弱学习器是否能产生一个强学习器。boosting的主要思想是迭代地将基础学习算法应用于输入数据的调整版本 [14]。**（串行训练）** boosting技术利用输入数据训练一个弱学习器，计算弱学习器的预测结果，选择误分类的训练样本，并使用包含上一轮训练中误分类实例的调整训练集来训练下一个弱学习器 [71]。被误分类的样本会得到更高的权重，使基础学习器更关注这些样本。如果基础分类器对特定样本有偏差，那么这些样本会获得更高的权重；因此，算法纠正了偏差。

#### 3.1.1 AdaBoost

由Freund和Schapire在1995年开发的 [69]，其基础学习器是具有单个分裂的决策树。因为这些决策树很短，通常被称为决策桩。最成功的AdaBoost实现是AdaBoost M1，用于二元分类任务 [74]。

缺点：容易过拟合

#### 3.1.2 GRADIENT BOOSTING 

主要使用决策树作为基础学习器来产生强健的集成分类器，也被称为梯度提升决策树（GBDT）。其核心思想是开发 #idea **与整个集成相关的损失函数的负梯度高度相关的基础学习器** [79]。

梯度提升的主要优势与其他 boosting 算法类似，它能够从输入数据中学习复杂的模式，因为它被训练来纠正先前模型的错误。然而，如果输入数据存在噪声，使用这种算法构建的模型可能会出现过拟合并模拟噪声[79]，[83]。该算法适用于数据集规模较小的应用[84]。

#### 3.1.3 XGBoost

XGBoost算法是基于决策树的集成方法，采用了梯度提升框架。它是一种可扩展且高度准确的算法，用于分类和回归应用。

本质上，优化被修改为目标函数的二次近似。此外，由于在 XGBoost 中引入了正则化项，它不容易出现过拟合现象[85]。与梯度提升算法类似，XGBoost 使用最大树深度、学习率和子采样来防止模型过拟合。

#### 3.1.4 LightGBM

LightGBM是梯度提升算法的高效实现，由微软的研究人员于2017年开发[88]。它可用于分类、排名和其他机器学习问题。LightGBM算法采用了两种新颖的方法，即Gradient-based One-Sided Sampling (GOSS)和Exclusive Feature Bundling (EFB)，确保算法训练速度快且准确率高。**在大数据集中表现较好。**

#### 3.1.5 CatBoost

CatBoost算法是梯度提升的一种实现，由Prokhorenkova等人于2017年提出[99]。该算法在训练阶段能够有效处理分类特征。CatBoost的一个显著改进是其能够执行无偏的梯度估计，从而减少了过拟合。

### 3.2 BAGGING 

基本上，自助聚合方法涉及将训练数据拆分为每个基本学习器使用随机抽样生成 b 不同子集来训练 b 个基本学习器。然后使用多数投票将这 b 个基本学习器组合起来获得强分类器 [27]。自助聚合的流程见算法4。随机森林是自助聚合技术的一种常见实现。

#### 3.2.1 Random Forest 

随机森林是一种集成算法，决策树是随机森林算法的主要组成部分 [107]。同时，该算法最早由Ho于1995年使用随机子空间方法开发，并在2001年由Breiman [51] 扩展。

### 3.3 Stacking

Meta-learning是机器学习的一部分，其中算法使用其他机器学习算法的输出进行训练，并在其他基础分类器的预测基础上做出更准确的预测。

## 4.在情绪分析上的应用（相关）

大部分都是简单的分类问题，很少和回归相关