
论文链接： https://link.springer.com/article/10.1007/s11704-019-8208-z

集成学习方法利用多种机器学习算法，基于通过对数据的多样性投影提取的特征产生弱预测结果，并将结果与各种投票机制融合，以实现比单独从任何组成算法获得的性能更好的性能。[1] **在Kaggle比赛中，集成学习取得了令人满意的成绩。**

### 1.监督集成分类

#### 1.1经典集成学习方法
Bagging[5]，AdaBoost[6]，随机森林[7]，随机子空间[8]，gradient boosting[9]
+ Bagging方法通过从训练数据集中随机采样来生成样本子集，然后使用这些获得的子集来训练用于集成的基本模型。
+ AdaBoost通过迭代调整样本的权重来关注被错误分类的样本，从而提高最终集成的基本模型的分类性能。
![image.png](https://cdn.jsdelivr.net/gh/Thomas333333/MyPostImage/Images/20240103164538.png)
+ 随机森林从两个角度训练多个决策树模型：样本维度和特征维度。因此，它通过集成多个决策树的投票结果来缓解决策树容易过度拟合的问题。
+ 随机子空间通过随机采样特征构建一组特征子空间，然后在这些子空间中训练基本分类器以生成多个结果，然后融合到最终结果中。
+ Gradient Boosting 随机采样以获得样本子集，然后构建和训练每个子学习器，以减少前一个子学习器产生的残差。因此，梯度增强可以使来自集成模型的最终残差之和足够小，从而迫使预测接近实际值。（欠拟合和过拟合的结合？）
![image.png](https://cdn.jsdelivr.net/gh/Thomas333333/MyPostImage/Images/20240103164821.png)

#### 1.2例子：

+ Ye等人[13]提出了一种分层采样方法，将特征分为两组：一组具有强信息，另一组具有弱信息。有了这两组，可以通过按比例从每组中采样来构建多个特征子空间。（避免样本分布不均匀）

许多基本模型被训练后用于预测，然后集成模型通过一致性函数融合来自这些基本模型的预测结果。接下来是选择子学习器的一些算法用例。
+ 周和唐[27]提出了使用比特串来表示树分类器在集合中的外观的GASEN-b算法，并对比特串采用遗传算法进行分类器选择。（用于子学习器数量较多的时候）
+ Dos Santos等人[31]将优化过程与动态选择策略相结合，以高精度选择最有信心的分类器子集。

**如何更好地整合基本模型的结果：**
+ 决策树——De Stefano等人[34]利用贝叶斯网络合并决策树集合的响应，提高了性能，大大减少了分类器的数量。
+ 聚类算法——Rahman和Verma[35]提出了一种面向聚类的分层集成分类算法，该算法集成了通过将聚类算法应用于多层数据而生成的分类器。
+ SVM——Zhang和Suganthan[37]将支持向量机引入到倾斜决策树集成[38]中，以帮助获得内部节点进行分类的测试超平面。
+ NN——周等人[39]提出了一种基于神经网络的集成方法，通过遗传算法更新网络的权值。

**理论知识：**
+ 王等[43]发现，随着基本分类器的模糊性越来越高，集成分类模型在处理具有复杂边界的数据集时可以获得更好的泛化能力
+ Yin等人[46]研究了集成模型的稀疏性和多样性，并引入了称为多样性贡献能力的概念用于分类器选择和权重调整。
+ 视频注释[84]和图像检索[87]


### 2.半监督集成分类

