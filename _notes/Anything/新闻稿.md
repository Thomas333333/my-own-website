AVS任务要求参赛者根据用户的文字输入进行合理的建模，从海量的视频数据集（140万视频片段）中找到符合描述的视频片段。团队利用嵌入模型和概念库等技术对数据集和文字查询进行处理，并按照文图匹配程度返回前1000个片段。这些片段中符合要求的越多，被评测的系统就性能更好。比如，对于文字查询“一个带着头盔的警察”，不仅要求从数据集中包含大量的包含人类的片段筛选出符合警察的片段，而且必须识别人物头上的物体是否存在“帽子”。数据集中存在许多易错选择，有人戴着头盔但不是警察，有的警察戴着头盔但不是帽子，这就要求参赛者的系统精确度足够高。
![image.png](https://cdn.jsdelivr.net/gh/Thomas333333/MyPostImage/Images/20230731095837.png)

由实习本科生何姜杉带领的AVS评测团队利用嵌入模型对视频内容和文字查询进行映射和编码，并利用扩散模型将跨模态任务转变成同模态之间的比较。在经过反复测试后，选择合适的排序融合技术，得到最终的自动排序结果。在此结果的基础上，引入基于量子理论的交互式排序融合方法，通过对之前结果的Top30个进行正负反馈信息，重新得到交互排序结果。团队所设计的系统在20个官方规定的赛题中，取得自动赛道推断准确率0.292，交互赛道推断准确率0.299，在与中国人民大学、早稻田大学、香港城市大学等学校和科研机构的队伍的竞争中取得第一名的好成绩。
